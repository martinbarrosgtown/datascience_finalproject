---
title: "Intro to Data Science Final Project"
author: "Cova, Langbehn, Villa & Barros" 
format: 
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    embed-resources: true
    theme: cosmo
editor: visual
---

# Model Construction and Implementation

## Setup and Data Loading: Packages

Repeat from earlier everything before step 3 is repeat.

```{r message=FALSE, warning=FALSE}
library(tidymodels)   
library(tidyverse)   
library(janitor)   
library(naniar)       
library(assertr)      
library(corrplot)
library(gridExtra)
library(reshape2)
library(glmnet)
library(dplyr)
library(vip)
library(ggplot2)

## Turn off scientific notation for readable numbers
options(scipen = 999)
```

## 1.1 Load the Data

Filter out missing values in our key predicted outcome as this will cause our predicted models to fail, it provides no additional data to our model. However, for missing values in our predictors, imputation will help ensure non-systemic missingness adversely affect the sample size of our model.

("../data/remittances.csv") \|\>

```{r message=FALSE, warning=FALSE}
remittances <- read_csv("../data/remittances.csv") |>
  clean_names() |>  
  filter(!is.na(remittances_gdp)) |>
  arrange(country_name, year) |> 
  group_by(country_name) |>
  mutate(
    gdp_lag = lag(gdp_per),
    deportations_lag = lag(deportations)
  ) |>
  ungroup()
```

## 1.2 Create Training and Testing Splits

```{r}
## Set seed for reproducibility
set.seed(20251211)

## Split data: 80% training, 20% testing
remit_split <- initial_split(data = remittances, prop = 0.8)
remit_train <- training(x = remit_split) 
remit_test <- testing(x = remit_split)
```

------------------------------------------------------------------------

# 

# STEP 3: Model Development

## 3.1 Cross-validation set up

To improve the accuracy of our estimated error rates, we set up a 10-fold cross validation with 5 repetitions since we have a relatively small number of observations within the training data. We create a recipe for the baseline model and process the full training data using parameter specification.

```{r}
remit_folds <- vfold_cv(data = remit_train, v = 10, repeats = 5)
```

## 3.2 Baseline Recipe

```{r}
## Recipe (baseline model)
recipe_baseline <- 
  recipe(remittances_gdp ~ stock + gdp_per + gdp_lag + unemployment + 
         dist_cap + terror + deportations + deportations_lag + 
         internet + inflation,
  data = remit_train) |>
step_impute_median(all_numeric_predictors()) |>
step_impute_mode(all_nominal_predictors()) |>
step_mutate(gdp_per = log(gdp_per + 1)) |>
step_normalize(all_numeric_predictors())

## Processing the full training data using parameter specification. 
bake(prep(recipe_baseline, training = remit_train), new_data = remit_train)
```

------------------------------------------------------------------------

# STEP 4: Model Comparison and Selection

In this section, we compare five different regression models to predict remittances as a percentage of GDP.

## 4.1 OLS Baseline Model

```{r}
## Linear model
lm_baseline <- linear_reg() |>
  set_mode(mode = "regression") |>
  set_engine(engine = "lm")

## Create workflow
lm_workflow <- workflow() |>
  add_recipe(recipe_baseline) |>
  add_model(lm_baseline)

## Fit model
lm_results <- lm_workflow |>
  fit_resamples(resamples = remit_folds)

## Collect RMSE
collect_metrics(lm_results)
```

The linear model establishes a baseline performance using ordinary least squares regression with no regularization. This gives us a benchmark to compare other models against.

The model

------------------------------------------------------------------------

## 4.2 Prepare Enhanced Recipe for Regularized Models

```{r}
#We clean remittances before folding due to missing values
train_data2 <- remit_train %>%
  filter(!is.na(remittances_gdp))   
remit_folds <- vfold_cv(train_data2, v = 10)

#At first, glmnet was throwing errors, so we need to create a recipe that forces
#your predictors into a form that ridge/lasso (glmnet) can handle, with no NA, no Inf / -Inf
#no constant columns, comparable scales across predictors.

recipe_glmnet <- recipe_baseline %>%
  step_mutate(across(all_numeric_predictors(),
                     ~ if_else(is.finite(.x), .x, NA_real_))) %>% 
  step_impute_median(all_numeric_predictors()) %>%               
  step_impute_mode(all_nominal_predictors()) %>%                 
  step_zv(all_predictors()) %>%                                  
  step_normalize(all_numeric_predictors())                       

ctrl <- control_grid(save_pred = TRUE, verbose = TRUE)
grid30 <- grid_regular(penalty(), levels = 30)

metrics1 <- metric_set(rmse)

#This control object makes errors visible and traceable
```

------------------------------------------------------------------------

## 4.3 RIDGE Regression

```{r message=FALSE, warning=FALSE}
#this defines ridge regression with a tuned penalty.
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

# build the workflow (recipe + model)
ridge_wf <- workflow() %>%
  add_recipe(recipe_glmnet) %>%
  add_model(ridge_spec)

#We tune the penalty using cross-validation
ridge_res <- tune_grid(
  ridge_wf,
  resamples = remit_folds,
  grid = grid30,
  metrics = metrics1,
  control = ctrl
)

best_ridge <- select_best(ridge_res, metric = "rmse")
final_ridge_wf <- finalize_workflow(ridge_wf, best_ridge)
ridge_fit <- fit(final_ridge_wf, data = train_data2)

# results
best_ridge
final_ridge_wf
ridge_fit 
```

Comment:

-   Penalty value with the lowest RMSE: best_ridge has a penalty \~ 0. The penalty disappears, with the model becomes almost identical to standard linear regression. The cross-validation procedure
-   Shows that adding regularization does not improve predictive performance relative to an unpenalized linear model.
-   Ridge_fit shows that ridge ≈ OLS Coefficients will be very similar to baseline linear model

Conclusion:

The ridge regression regularization indicated increasing deviance as the penalty decreased, with cross-validation selecting a penalty effectively equal to zero. This suggests that the unpenalized linear model already provides an optimal fit for the data.

------------------------------------------------------------------------

## 4.4 LASSO Regression

```{r message=FALSE, warning=FALSE}
# we specify the LASSO model
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

# we build the workflow - preprocess to prevent leakage
lasso_wf <- workflow() %>%
  add_recipe(recipe_glmnet) %>%
  add_model(lasso_spec)

# Tune lambda penalty  using cross-validation
lasso_res <- tune_grid(
  lasso_wf,
  resamples = remit_folds,
  grid = grid30,
  metrics = metrics1,
  control = ctrl
)

# choose the best lambda (lowest RMSE)
best_lasso <- select_best(lasso_res, metric = "rmse")
# finalize the workflow
final_lasso_wf <- finalize_workflow(lasso_wf, best_lasso)
# fit the final LASSO model on all training data
lasso_fit <- fit(final_lasso_wf, data = train_data2)

best_lasso
final_lasso_wf
lasso_fit

tidy(lasso_fit) %>%
  filter(term != "(Intercept)") %>%
  filter(estimate != 0) %>%
  arrange(desc(abs(estimate)))
```

Calculating the RMSE

```{r}
# RMSE comparison
bind_rows(
  collect_metrics(ridge_res) |>
    filter(.metric == "rmse") |>
    inner_join(best_ridge, by = "penalty") |>
    mutate(model = "ridge"),
  collect_metrics(lasso_res) |>
    filter(.metric == "rmse") |>
    inner_join(best_lasso, by = "penalty") |>
    mutate(model = "lasso")
) |>
  select(model, penalty, mean, std_err)
```

Comment

Cross-validation selected a non-zero penalty for the LASSO model, indicating that it improves predictive performance. The LASSO regularization path shows a small set of predictors entering the model as the penalty decreases, highlighting its role as a variable selection method.

Comment on selected predictors

The LASSO model selected a set of nine predictors. Remittance intensity is negatively associated with GDP per capita and macroeconomic instability, while unemployment, deportations, and internet access exhibit positive relationships, consistent with counter-cyclical and transaction-cost mechanisms.

Why LASSO is useful:

-   The dataset benefits from variable selection (LASSO), not from coefficient stabilization (RIDGE).

------------------------------------------------------------------------

## 4.5 Random Forest

We use this model a regularization model, to reduce variance error by reducing the important of less important predictors. It is a bagging algorithm which considers each split and divides it into only useful predictors - It uses two hyper paramaters `mtry` which considers x predictors in each split (it can be tuned to optimal value of useful predictors. and `min_n` to stop spliting the data

```{r message=FALSE, warning=FALSE}
# For missing values inside the dependent variable 
remit_train_clean <- remit_train |>
  filter(!is.na(remittances_gdp))

# Smaller CV for run time optimization 
rf_folds <- vfold_cv(remit_train_clean, v = 5)

## Given the high degree of missingness a recipe that accounts for nas will avoid it from breaking down using median imputation. 
recipe_alt <- 
  recipe(remittances_gdp ~ stock + gdp_per + gdp_lag + unemployment + 
         dist_cap + terror + deportations + deportations_lag + 
         internet + inflation + country_name,
  data = remit_train_clean) |>
update_role(country_name, new_role = "id") |>
step_impute_median(all_numeric_predictors()) |>
step_mutate(gdp_per = log(gdp_per + 1)) |>
step_normalize(all_numeric_predictors())

bake(prep(recipe_alt, training = remit_train_clean), new_data = remit_train_clean)

## Creating a Random Forest Model set up for tuning
rf_mod <- rand_forest(
  trees = tune(),
  mtry = tune(),
  min_n = tune()) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "ranger", 
             importance = "impurity", 
             num.threads = 4)

## Creating a workflow. Need to use alternative specific because it accounts for missingness which will lead the model to fail. 
rf_wf <- workflow() |>
  add_recipe(recipe_alt) |>
  add_model(rf_mod)

## Finalize parameters

rf_params <- rf_wf |> 
  extract_parameter_set_dials() |>
  finalize(remit_train_clean)

rf_params


## tuning grid 
rf_grid <- grid_max_entropy(
  rf_params, 
  size = 20 )

## Tuning it within cross validation using our hyperparamters.
rf_tuned <- rf_wf |>
  tune_grid(
    resamples = rf_folds,
    grid = rf_grid,
    control = control_grid(save_pred = TRUE))

## Measuring the RMSE 
rf_tuned |>
  collect_metrics()

rf_tuned |> 
  show_best(metric = "rmse", n = 10)

## selecting the best specification and fit it to the full training data. 
best_rf <-rf_tuned |>
  select_best(metric = "rmse")

final_rf_wf <- rf_wf |> 
  finalize_workflow(best_rf)

final_rf_fit <- final_rf_wf |> 
  fit(data = remit_train_clean)

# Variable importance 
final_rf_fit |> 
  extract_fit_parsnip() |> 
  vip(num_features = 10)
```

------------------------------------------------------------------------

## 4.6 K-Nearest Neighbors (KNN)

KNN predicts remittances by averaging the values of the K most similar country-year observations.

```{r}
## Create lagged variables by country
remit_train_lagged <- remit_train |>
  arrange(country_name, year) |>
  group_by(country_name) |>
  mutate(
    gdp_lag = lag(gdp_per),
    deportations_lag = lag(deportations)
  ) |>
  ungroup()

## Remove missing values
remit_train_clean_knn <- remit_train_lagged |>
  select(remittances_gdp, stock, gdp_per, unemployment, dist_cap, 
         terror, deportations, internet, inflation, gdp_lag, deportations_lag) |>
  drop_na()

## Cross-validation setup
set.seed(20251211)
knn_folds <- vfold_cv(data = remit_train_clean_knn, v = 10, repeats = 5)

## Recipe: log transform and normalize
recipe_knn <- 
  recipe(remittances_gdp ~ ., data = remit_train_clean_knn) |>
  step_mutate(gdp_per = log(gdp_per + 1)) |>
  step_normalize(all_numeric_predictors())

## Model: KNN with tunable K
knn_mod <- 
  nearest_neighbor(neighbors = tune()) |>
  set_engine("kknn") |>
  set_mode("regression")

## Grid: test K from 1 to 99
knn_grid <- grid_regular(neighbors(range = c(1, 99)), levels = 10)

## Workflow
knn_workflow <- 
  workflow() |>
  add_recipe(recipe_knn) |>
  add_model(knn_mod)

## Tune
knn_results <- 
  knn_workflow |>
  tune_grid(
    resamples = knn_folds,
    grid = knn_grid,
    metrics = metric_set(rmse, rsq)
  )

## Results
knn_results |> collect_metrics()
knn_results |> show_best(metric = "rmse")
knn_results |> autoplot()

## Select best
best_k <- select_best(knn_results, metric = "rmse")
final_knn_wf <- knn_workflow |> finalize_workflow(best_k)
```

------------------------------------------------------------------------

## 4.7 Which Model is the best?

```{r}
## Compare all models
tibble(
  Model = c("OLS", "Ridge", "LASSO", "Random Forest", "KNN"),
  Error = c(
    collect_metrics(lm_results) |> filter(.metric == "rmse") |> pull(mean),
    collect_metrics(ridge_res) |> filter(.metric == "rmse") |> inner_join(best_ridge, by = "penalty") |> pull(mean),
    collect_metrics(lasso_res) |> filter(.metric == "rmse") |> inner_join(best_lasso, by = "penalty") |> pull(mean),
    collect_metrics(rf_tuned) |> filter(.metric == "rmse") |> slice_min(mean) |> pull(mean),
    collect_metrics(knn_results) |> filter(.metric == "rmse") |> slice_min(mean) |> pull(mean)
  )
) |>
  arrange(Error)
```

Dot plot

```{r}
tibble(
  Model = c("OLS", "Ridge", "LASSO", "Random Forest", "KNN"),
  Error = c(
    collect_metrics(lm_results) |> filter(.metric == "rmse") |> pull(mean),
    collect_metrics(ridge_res) |> filter(.metric == "rmse") |> inner_join(best_ridge, by = "penalty") |> pull(mean),
    collect_metrics(lasso_res) |> filter(.metric == "rmse") |> inner_join(best_lasso, by = "penalty") |> pull(mean),
    collect_metrics(rf_tuned) |> filter(.metric == "rmse") |> slice_min(mean) |> pull(mean),
    collect_metrics(knn_results) |> filter(.metric == "rmse") |> slice_min(mean) |> pull(mean)
  )
) |>
  ggplot(aes(x = Error, y = reorder(Model, -Error))) +
  geom_point(size = 4, color = "steelblue") +
  geom_segment(aes(x = 0, xend = Error, y = Model, yend = Model), color = "gray70") +
  labs(title = "Model Performance", 
       subtitle = "Left is better",
       x = "Prediction Error", 
       y = NULL,
      caption = "Source: World Bank Development Indicators (1994-2024)") +
  theme_minimal()
```

# STEP 5: Final Model Evaluation

The best model is **KNN** (lowest error: 3.05). Now we test it on completely new data.

## 5.1 Test the Best Model on New Data

```{r}
## Prepare test data
remit_test_clean_knn <- remit_test |>
  select(
    country_name, year,
    remittances_gdp, stock, gdp_per, unemployment, dist_cap,
    terror, deportations, internet, inflation, gdp_lag, deportations_lag
  ) |>
  drop_na()
```

```{r}
## Finalize & fit KNN on all training data
best_k <- select_best(knn_results, metric = "rmse")
final_knn_wf <- finalize_workflow(knn_workflow, best_k)

final_knn_fit <- final_knn_wf |>
  fit(data = remit_train_clean_knn)
```

```{r}
## Predict on test 
test_predictions <- final_knn_fit |>
  augment(new_data = remit_test_clean_knn)
```

## 5.2 Country-Specific Performance & Residual Analysis

```{r}
## Look at countries of interest
countries_of_interest <- c(
  "Nicaragua", "El Salvador", "Honduras",
  "Guatemala", "Haiti", "India", "Mexico"
)
```

```{r}
## Calculate RMSE by country
rmse_tbl <- test_predictions |>
  filter(country_name %in% countries_of_interest) |>
  group_by(country_name) |>
  summarise(
    RMSE = rmse_vec(truth = remittances_gdp, estimate = .pred),
    MAE  = mae_vec(truth = remittances_gdp, estimate = .pred),
    R2   = rsq_vec(truth = remittances_gdp, estimate = .pred),
    Avg_Remittances_GDP = mean(remittances_gdp, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) |>
  arrange(RMSE)

avg_perf_interest <- test_predictions |>
  filter(country_name %in% countries_of_interest) |>
  summarise(
    RMSE = rmse_vec(truth = remittances_gdp, estimate = .pred),
    MAE  = mae_vec(truth = remittances_gdp, estimate = .pred),
    R2   = rsq_vec(truth = remittances_gdp, estimate = .pred),
    n = n()
  )

rmse_tbl
avg_perf_interest
```

```{r}
## Visualize actual vs. predicted
test_predictions |>
  filter(country_name %in% countries_of_interest) |>
  ggplot(aes(x = remittances_gdp, y = .pred)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  facet_wrap(~ country_name) +
  labs(
    title = "KNN: Test Set Performance by Country",
    subtitle = "Points near the diagonal = better predictions",
    x = "Actual Remittances (% GDP)",
    y = "Predicted",
    caption = "Source: World Bank Development Indicators (1994-2024)") +
  theme_minimal()
```

## 5.3 Key Insights and Policy Implications

**Insights** - Strong performance for large, diversified economies: Mexico and India show very low prediction error (RMSE \< 0.5) and very high explanatory power (R² ≈ 1). The model performs best where remittances respond predictably to macroeconomic fundamentals and migration stocks. - Moderate accuracy for mid-dependence countries: Haiti and Nicaragua exhibit moderate RMSE (≈ 1.6–2.6) and reasonably strong fit. - Weak performance for highly remittance-dependent economies - Guatemala, El Salvador, and Honduras have the highest RMSE values (≈ 5–7) and low R², despite being central to U.S.–migration and deportation policy discussions. - These countries rely heavily on remittances (≈ 15–20% of GDP), making flows more sensitive to household-level shocks not captured by the model.

```{r}
## Plot residuals
residuals_df <- test_predictions |>
  filter(country_name %in% countries_of_interest) |>
  mutate(residual = .pred - remittances_gdp)

ggplot(residuals_df, aes(x = year, y = residual)) +
  geom_point(alpha = 0.6) +
  geom_smooth(alpha = 0.4, se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  facet_wrap(~ country_name, scales = "free_y") +
  labs(
    title = "KNN Residuals Over Time by Country",
    subtitle = "Positive values = overprediction; negative = underprediction",
    x = "Year",
    y = "Residual (Predicted − Actual)",
    caption = "Source: World Bank Development Indicators (1994-2024)") +
  theme_minimal()
```

**Interpretation: Residuals Over Time by Country** - Large, diversified economies (Mexico, India) - Residuals are tightly centered around zero with very small magnitudes. - No clear time trend or structural bias is evident. - This reinforces that the KNN model captures remittance dynamics well when flows scale predictably with macroeconomic and migration variables. - Highly remittance-dependent Central American countries show systematic error - El Salvador and Honduras exhibit large, persistent negative residuals in later years, indicating systematic underprediction. - These patterns suggest the model fails to capture structural increases in remittance dependence over time.
